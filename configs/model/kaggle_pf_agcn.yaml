# configs/model/kaggle_pf_agcn.yaml
# PF-AGCN variant sized for Kaggle T4 VRAM while retaining full prior support.

seq_embeddings:
  cache_path: ${system.cache_dir}/seq_embeddings
  backend: esm
  feature_dim: 256
  seq_len: 1024
  dtype: float16

prot_prior:
  enabled: true
  method: cosine
  blastp_exec: null
  evalue_threshold: 1.0e-05
  cache_path: ${system.cache_dir}/priors/protein
  top_p_mass: 0.9
  temperature: 1.0
  symmetric: true
  sim_threshold: 0.3

go_prior:
  enabled: true
  cache_path: ${system.cache_dir}/priors/go
  top_p_mass: 0.9
  temperature: 1.0
  symmetric: true

dccn:
  channels: 96
  kernel_size: 3
  dilation: 2
  dropout: 0.1

seq_gating:
  shared_dim: 192
  esm_dim: ${..seq_embeddings.feature_dim}
  dccn_channels: ${..dccn.channels}
  attn_hidden: 192
  dropout: 0.1

seq_final:
  metric_dim: 128
  graph_dim: ${..seq_gating.shared_dim}
  function_dim: ${..seq_gating.shared_dim}
  protein_dim: ${..seq_gating.shared_dim}
graph:
  structure: decoupled
  protein_stacks: 2
  function_stacks: 2
  alternating_depth: 4

adaptive_protein:
  feature_dim: ${..seq_gating.shared_dim}
  attention_dim: 192
  steps: 2
  top_p_mass: 0.9
  temperature: 1.0
  dropout: 0.1

adaptive_function:
  feature_dim: ${..seq_gating.shared_dim}
  attention_dim: 192
  steps: 2
  top_p_mass: 0.9
  temperature: 1.0
  dropout: 0.1

head:
  feature_dim: ${..seq_gating.shared_dim}
  dropout: 0.1
  attn_hidden: ${..seq_gating.attn_hidden}

loss:
  name: bce_with_logits
  pos_weight: null
