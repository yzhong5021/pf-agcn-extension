# @package _global_
# configs/train/kaggle_train.yaml
# Training schedule calibrated for Kaggle T4 (16GB VRAM, limited CPU threads).

training:
  seed: 10
  max_epochs: 15
  gradient_clip: 1.0
  precision: 16
  accumulate_batches: 1
  log_interval: 20
  devices: 1
  accelerator: gpu
  strategy: null
  deterministic: true
  enable_progress_bar: true
  limit_train_batches: null
  limit_val_batches: null
  fast_dev_run: false

optimizer:
  name: adamw
  lr: 0.0006
  weight_decay: 0.01
  betas: [0.9, 0.999]

scheduler:
  name: cosine
  warmup_epochs: 3
  min_lr: 0.00001

data:
  train_manifest: null
  val_manifest: null
  test_manifest: null
  batch_size: 12
  num_workers: 2
  pin_memory: true
  drop_last: false

evaluation:
  metric: fmax
  val_interval: 1
  threshold_grid: [0.2, 0.4, 0.6, 0.8]
