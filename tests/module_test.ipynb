{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1657984f",
   "metadata": {},
   "source": [
    "# Module Testing\n",
    "\n",
    "Modules tested:\n",
    "- Toy data generator\n",
    "- MockESM\n",
    "- DCCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f09eb097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "root = os.path.abspath('..')\n",
    "sys.path.insert(0, root)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# models\n",
    "from modules.mock_esm import MockESM\n",
    "from modules.dccn import DCCN_1D\n",
    "\n",
    "# toy data generator\n",
    "from tests.generate_toy_data import generate_toy_data\n",
    "\n",
    "# linear probe  \n",
    "from tests.dccn_probe import LinearProbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8241fe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "n_go_terms = 5\n",
    "min_len = 10\n",
    "max_len = 20\n",
    "\n",
    "df, go_vocab, labels = generate_toy_data(\n",
    "    n_samples=n_samples, \n",
    "    n_go_terms=n_go_terms, \n",
    "    min_len=min_len, \n",
    "    max_len=max_len, \n",
    "    seed = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ccac632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GO:0000001', 'GO:0000002', 'GO:0000003', 'GO:0000004', 'GO:0000005']\n",
      "EPNDRGVQFIINSHNI; ['GO:0000002', 'GO:0000001']\n",
      "SFRQHAVRHVGEKGLDAMAK; ['GO:0000005', 'GO:0000001']\n",
      "LPMIINYSELPRN; ['GO:0000005', 'GO:0000003']\n"
     ]
    }
   ],
   "source": [
    "print(go_vocab)\n",
    "for i in range(3):\n",
    "    print(f'{df.iloc[i]['sequence']}; {df.iloc[i]['go_terms']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8462c2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 20\n",
      "Sequence dimensions\n",
      "    Train: torch.Size([10, 20])\n",
      "    Val: torch.Size([20, 20])\n",
      "    Test: torch.Size([20, 20])\n",
      "Label dimensions\n",
      "    Train: torch.Size([10, 5])\n",
      "    Val: torch.Size([20, 5])\n",
      "    Test: torch.Size([20, 5])\n"
     ]
    }
   ],
   "source": [
    "# Convert sequences to indices\n",
    "AA_TO_IDX = {aa: i for i, aa in enumerate('ACDEFGHIKLMNPQRSTVWY')}\n",
    "train_prop = 0.8\n",
    "test_val_prop = 0.5\n",
    "\n",
    "def sequence_to_indices(sequence):\n",
    "    return [AA_TO_IDX[aa] for aa in sequence]\n",
    "\n",
    "def process_sequences(sequences, max_len):\n",
    "    \"\"\"\n",
    "    convert sequences to indices, pad to max length\n",
    "    \"\"\"\n",
    "\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        tokens = sequence_to_indices(seq)\n",
    "        if len(tokens) < max_len:\n",
    "            tokens.extend([20] * (max_len - len(tokens)))  # 20 is the padding index\n",
    "        else:\n",
    "            tokens = tokens[:max_len]  # truncate to max length\n",
    "        padded.append(tokens)\n",
    "    \n",
    "    return torch.tensor(padded, dtype=torch.long)\n",
    "\n",
    "max_seq_len = max(len(seq) for seq in df['sequence'])\n",
    "print(f\"Max sequence length: {max_seq_len}\")\n",
    "\n",
    "train_seqs, test_val_seqs = train_test_split(df['sequence'], test_size=train_prop)\n",
    "val_seqs, test_seqs = train_test_split(test_val_seqs, test_size=test_val_prop)\n",
    "\n",
    "train_lab, test_val_lab = train_test_split(labels, test_size=train_prop)\n",
    "val_lab, test_lab = train_test_split(test_val_lab, test_size=test_val_prop)\n",
    "\n",
    "train_seqs_final = process_sequences(train_seqs, max_seq_len)\n",
    "val_seqs_final = process_sequences(val_seqs, max_seq_len)\n",
    "test_seqs_final = process_sequences(test_seqs, max_seq_len)\n",
    "\n",
    "train_labels_final = torch.tensor(train_lab, dtype=torch.float32)\n",
    "val_labels_final = torch.tensor(val_lab, dtype=torch.float32)\n",
    "test_labels_final = torch.tensor(test_lab, dtype=torch.float32)\n",
    "\n",
    "print(f'Sequence dimensions')\n",
    "print(f'    Train: {train_seqs_final.shape}')\n",
    "print(f'    Val: {val_seqs_final.shape}')\n",
    "print(f'    Test: {test_seqs_final.shape}')\n",
    "print(f'Label dimensions')\n",
    "print(f'    Train: {train_labels_final.shape}')\n",
    "print(f'    Val: {val_labels_final.shape}')\n",
    "print(f'    Test: {test_labels_final.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a9f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimal hyperparameters\n",
    "embed_len = 16      # ESM embedding dimension (for each amino acid)\n",
    "hidden_len = 32     # MockESM hidden layer\n",
    "proj_len = 24       # ESM projection dimension\n",
    "go_dim = n_go_terms # output dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "543c7ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MockESM dimensions: 16 -> 32 -> 24\n",
      "DCCN dimensions: 24 channels with (1, 2, 4, 8) dilations\n",
      "LinearProbe dimensions: 24 -> 5\n",
      "\n",
      " Number of parameters:\n",
      "    MockESM: 1672\n",
      "    DCCN: 16512\n",
      "    LinearProbe: 125\n",
      "    Total: 18309\n"
     ]
    }
   ],
   "source": [
    "# initialize models\n",
    "mock_esm = MockESM(\n",
    "    seq_len=max_seq_len, \n",
    "    hidden_len=hidden_len, \n",
    "    embed_len=embed_len, \n",
    "    proj_len=proj_len\n",
    ")\n",
    "\n",
    "dccn = DCCN_1D(embed_len=proj_len)\n",
    "\n",
    "linear_probe = LinearProbe(in_dim=proj_len, go_dim=go_dim)\n",
    "\n",
    "print(f'MockESM dimensions: {embed_len} -> {hidden_len} -> {proj_len}')\n",
    "print(f'DCCN dimensions: {proj_len} channels with {dccn.dilations} dilations')\n",
    "print(f'LinearProbe dimensions: {proj_len} -> {go_dim}')\n",
    "\n",
    "def param_count(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'\\n Number of parameters:')\n",
    "print(f'    MockESM: {param_count(mock_esm)}')\n",
    "print(f'    DCCN: {param_count(dccn)}')\n",
    "print(f'    LinearProbe: {param_count(linear_probe)}')\n",
    "print(f'    Total: {param_count(mock_esm) + param_count(dccn) + param_count(linear_probe)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c5c5281",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_seqs = train_seqs_final[:10]\n",
    "test_batch_labels = train_labels_final[:10]\n",
    "models = {\n",
    "    'mock_esm': mock_esm,\n",
    "    'dccn': dccn,\n",
    "    'linear_probe': linear_probe\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17829438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial dimensions: torch.Size([10, 20])\n",
      "dimensions after mock_esm: torch.Size([10, 20, 24])\n",
      "dimensions after dccn: torch.Size([10, 20, 24])\n",
      "dimensions after mean: torch.Size([10, 24])\n",
      "dimensions after probe: torch.Size([10, 5])\n",
      "tensor(0.7973, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) True\n"
     ]
    }
   ],
   "source": [
    "# test step\n",
    "def test_step(seqs, labels, models):\n",
    "    mock_esm, dccn, probe = models['mock_esm'], models['dccn'], models['linear_probe']\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for m in (mock_esm, dccn, probe):\n",
    "        m.train()                      # enable grads\n",
    "        m.zero_grad(set_to_none=True)\n",
    "\n",
    "    # forward\n",
    "    print(f\"initial dimensions: {seqs.shape}\")\n",
    "    z = mock_esm(seqs)\n",
    "    print(f\"dimensions after mock_esm: {z.shape}\")\n",
    "    z = dccn(z)\n",
    "    print(f\"dimensions after dccn: {z.shape}\")\n",
    "    z = z.mean(dim=1)\n",
    "    print(f\"dimensions after mean: {z.shape}\")\n",
    "    logits = probe(z)\n",
    "    print(f\"dimensions after probe: {logits.shape}\")\n",
    "\n",
    "    # backward (without optimizer step)\n",
    "    loss = loss_fn(logits, labels.float())\n",
    "    loss.backward()\n",
    "\n",
    "    # check if gradients exist\n",
    "    grad_ok = all(\n",
    "        p.grad is not None for m in (mock_esm, dccn, probe) for p in m.parameters() if p.requires_grad\n",
    "    )\n",
    "\n",
    "    return loss, grad_ok\n",
    "\n",
    "loss, grad_ok = test_step(test_batch_seqs, test_batch_labels, models)\n",
    "print(loss, grad_ok)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfagcn",
   "language": "python",
   "name": "pfagcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
